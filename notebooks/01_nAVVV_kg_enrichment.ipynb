{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Scholar KG Enrichment: USES_DATASET\n",
    "\n",
    "### Overview\n",
    "In this notebook, we will:\n",
    "1. Fetch ML-related papers from the Semantic Scholar API.\n",
    "2. Perform Exploratory Data Analysis (EDA).\n",
    "3. Apply a simple NLP pipeline (spaCy) to extract dataset mentions from abstracts.\n",
    "4. Build a Neo4j knowledge graph with `USES_DATASET` relationships.\n",
    "5. Query and visualize insights about dataset usage.\n",
    "\n",
    "**Author**: Jo√£o Pereira  \n",
    "**Date**: 2025-03-01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 1. Imports and Initial Setup\n",
    "# ============================================\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "\n",
    "# For NLP\n",
    "import spacy\n",
    "\n",
    "# For Neo4j\n",
    "from py2neo import Graph, Node, Relationship\n",
    "\n",
    "# Load spaCy model (English)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Configure display\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 5)\n",
    "\n",
    "# Neo4j Credentials\n",
    "NEO4J_URI = \"bolt://localhost:7687\"  # or \"bolt://neo4j:7687\" if in Docker\n",
    "NEO4J_USER = \"neo4j\"\n",
    "NEO4J_PASS = \"password123\"\n",
    "\n",
    "try:\n",
    "    graph = Graph(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASS))\n",
    "    print(\"Connected to Neo4j successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to Neo4j: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 2. Fetch Data from Semantic Scholar\n",
    "# ============================================\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Semantic Scholar API does NOT require an API Key for basic queries\n",
    "API_URL = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
    "\n",
    "def fetch_papers(query, offset=0, limit=100):\n",
    "    \"\"\"\n",
    "    Fetches papers from the Semantic Scholar API given a query.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"query\": query,\n",
    "        \"fields\": \"paperId,title,abstract,year,authors,referenceCount,citationCount\",\n",
    "        \"offset\": offset,\n",
    "        \"limit\": limit\n",
    "    }\n",
    "    \n",
    "    # No API key required for basic search\n",
    "    resp = requests.get(API_URL, params=params)\n",
    "\n",
    "    print(f\"üîπ Requesting: {resp.url}\")  # Debugging: See the full request URL\n",
    "    print(f\"üîπ Status Code: {resp.status_code}\")  # Debugging: Print response status\n",
    "    \n",
    "    if resp.status_code == 200:\n",
    "        resp_data = resp.json()\n",
    "        return resp_data.get(\"data\", [])  # Extract papers from response\n",
    "    else:\n",
    "        print(f\"‚ùå API Error {resp.status_code}: {resp.text}\")\n",
    "        return []\n",
    "\n",
    "# Test with a single request before looping\n",
    "papers_batch = fetch_papers(\"machine learning\", offset=0, limit=10)\n",
    "print(f\"‚úÖ Papers fetched: {len(papers_batch)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "API_URL = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
    "\n",
    "def fetch_papers(query, offset=0, limit=100, max_retries=5):\n",
    "    \"\"\"\n",
    "    Fetches papers from the Semantic Scholar API given a query.\n",
    "    Implements automatic retrying when rate-limited (HTTP 429).\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"query\": query,\n",
    "        \"fields\": \"paperId,title,abstract,year,authors,referenceCount,citationCount\",\n",
    "        \"offset\": offset,\n",
    "        \"limit\": limit\n",
    "    }\n",
    "\n",
    "    attempt = 0\n",
    "    wait_time = 5  # Start with 5-second delay for retries\n",
    "\n",
    "    while attempt < max_retries:\n",
    "        resp = requests.get(API_URL, params=params)\n",
    "\n",
    "        print(f\"üîπ Requesting: {resp.url}\")\n",
    "        print(f\"üîπ Status Code: {resp.status_code}\")\n",
    "\n",
    "        if resp.status_code == 200:\n",
    "            resp_data = resp.json()\n",
    "            print(f\"‚úÖ Successfully fetched {len(resp_data.get('data', []))} papers\")\n",
    "            return resp_data.get(\"data\", [])\n",
    "\n",
    "        elif resp.status_code == 429:\n",
    "            print(f\"‚ö†Ô∏è Rate limit exceeded. Retrying in {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)  # Wait before retrying\n",
    "            wait_time *= 2  # Exponential backoff\n",
    "            attempt += 1\n",
    "\n",
    "        else:\n",
    "            print(f\"‚ùå API Error {resp.status_code}: {resp.text}\")\n",
    "            return []\n",
    "\n",
    "    print(\"‚ùå Maximum retries reached. Skipping batch.\")\n",
    "    return []\n",
    "\n",
    "# Fetching multiple batches\n",
    "all_papers = []\n",
    "num_batches = 10  \n",
    "\n",
    "for i in range(num_batches):\n",
    "    offset = i * 100\n",
    "    papers_batch = fetch_papers(\"machine learning\", offset=offset, limit=100)\n",
    "\n",
    "    all_papers.extend(papers_batch)\n",
    "    print(f\"‚úÖ Fetched {len(papers_batch)} papers in batch {i+1}, Total: {len(all_papers)}\")\n",
    "\n",
    "    time.sleep(3)  # Add a small delay between successful requests\n",
    "\n",
    "# Save results\n",
    "with open(\"/data/raw/papers_ml.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_papers, f, indent=4)\n",
    "\n",
    "print(f\"‚úÖ Total papers fetched: {len(all_papers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 3. Exploratory Data Analysis (EDA)\n",
    "# ============================================\n",
    "\n",
    "df = pd.DataFrame(all_papers)\n",
    "print(\"DataFrame shape:\", df.shape)\n",
    "df.head()\n",
    "\n",
    "# Basic stats\n",
    "df.info()\n",
    "display(df.describe())\n",
    "\n",
    "# Distribution of years\n",
    "year_counts = df[\"year\"].value_counts().sort_index()\n",
    "year_counts.plot(kind=\"bar\", title=\"Publication Year Distribution\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Count of Papers\")\n",
    "plt.show()\n",
    "\n",
    "# Check some sample abstracts\n",
    "for idx, row in df.head(3).iterrows():\n",
    "    print(\"Title:\", row.get(\"title\"))\n",
    "    print(\"Abstract:\", row.get(\"abstract\"))\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "- We have ~1000 papers about \"machine learning.\"\n",
    "- Some might not have abstracts (missing fields), so we need to handle that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 4. NLP to Extract Dataset Mentions\n",
    "# ============================================\n",
    "\n",
    "known_datasets = [\n",
    "    \"MNIST\", \"ImageNet\", \"CIFAR-10\", \"CIFAR-100\", \"COCO\", \"Fashion-MNIST\",\n",
    "    \"OpenImages\", \"Cityscapes\", \"Pascal VOC\"\n",
    "    # add more known dataset names if needed\n",
    "]\n",
    "\n",
    "def extract_datasets_from_text(abstract_text, dataset_list=None):\n",
    "    if not abstract_text:\n",
    "        return []\n",
    "    \n",
    "    found = []\n",
    "    lower_text = abstract_text.lower()\n",
    "    for ds in dataset_list or []:\n",
    "        if ds.lower() in lower_text:\n",
    "            found.append(ds)\n",
    "    \n",
    "    # Alternatively, spaCy NER could be used for more advanced detection:\n",
    "    # doc = nlp(abstract_text)\n",
    "    # for ent in doc.ents:\n",
    "    #     # if ent.label_ == \"PRODUCT\" or something relevant\n",
    "    #     # maybe cross-check if ent.text is in a dataset dictionary\n",
    "    \n",
    "    return list(set(found))\n",
    "\n",
    "paper_dataset_map = []\n",
    "\n",
    "for paper in all_papers:\n",
    "    paper_id = paper.get(\"paperId\", \"\")\n",
    "    paper_title = paper.get(\"title\", \"\")\n",
    "    abstract = paper.get(\"abstract\", \"\")\n",
    "    \n",
    "    datasets_found = extract_datasets_from_text(abstract, known_datasets)\n",
    "    for ds in datasets_found:\n",
    "        paper_dataset_map.append({\n",
    "            \"paperId\": paper_id,\n",
    "            \"title\": paper_title,\n",
    "            \"dataset\": ds\n",
    "        })\n",
    "\n",
    "df_map = pd.DataFrame(paper_dataset_map)\n",
    "print(\"Rows in paper-dataset mapping:\", df_map.shape[0])\n",
    "df_map.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate Extraction\n",
    "- We can do a quick manual check on a few samples to see if the extraction is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 5. Build Knowledge Graph in Neo4j\n",
    "# ============================================\n",
    "\n",
    "# Optional: Wipe the existing DB (DANGER: deletes everything!)\n",
    "# graph.run(\"MATCH (n) DETACH DELETE n\")\n",
    "\n",
    "for _, row in df_map.iterrows():\n",
    "    p_id = row[\"paperId\"]\n",
    "    p_title = row[\"title\"]\n",
    "    ds_name = row[\"dataset\"]\n",
    "    \n",
    "    paper_node = Node(\"Paper\", paperId=p_id, title=p_title)\n",
    "    dataset_node = Node(\"Dataset\", name=ds_name)\n",
    "    \n",
    "    # MERGE ensures uniqueness\n",
    "    graph.merge(paper_node, \"Paper\", \"paperId\")\n",
    "    graph.merge(dataset_node, \"Dataset\", \"name\")\n",
    "    \n",
    "    rel = Relationship(paper_node, \"USES_DATASET\", dataset_node)\n",
    "    graph.merge(rel, \"Paper\", \"paperId\")\n",
    "\n",
    "print(\"Knowledge Graph construction complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 6. Querying & Insights\n",
    "# ============================================\n",
    "\n",
    "# 6.1 Top 5 Datasets\n",
    "query_top5 = \"\"\"\n",
    "MATCH (p:Paper)-[:USES_DATASET]->(d:Dataset)\n",
    "RETURN d.name AS datasetName, COUNT(p) AS usageCount\n",
    "ORDER BY usageCount DESC\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "res_top5 = graph.run(query_top5).data()\n",
    "df_top5 = pd.DataFrame(res_top5)\n",
    "print(\"Top 5 Datasets:\")\n",
    "display(df_top5)\n",
    "\n",
    "# Bar chart\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.barplot(data=df_top5, x=\"datasetName\", y=\"usageCount\")\n",
    "plt.title(\"Top 5 Datasets Mentioned\")\n",
    "plt.xlabel(\"Dataset\")\n",
    "plt.ylabel(\"Number of Papers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 Yearly Trend for a Chosen Dataset\n",
    "chosen_dataset = \"ImageNet\"\n",
    "query_trend = f\"\"\"\n",
    "MATCH (p:Paper)-[:USES_DATASET]->(d:Dataset {{name: '{chosen_dataset}'}})\n",
    "RETURN p.year AS year, COUNT(p) AS usageCount\n",
    "ORDER BY year\n",
    "\"\"\"\n",
    "res_trend = graph.run(query_trend).data()\n",
    "df_trend = pd.DataFrame(res_trend)\n",
    "df_trend = df_trend.dropna(subset=[\"year\"])  # drop None years if any\n",
    "\n",
    "if not df_trend.empty:\n",
    "    plt.figure(figsize=(8,4))\n",
    "    sns.lineplot(data=df_trend, x=\"year\", y=\"usageCount\", marker=\"o\")\n",
    "    plt.title(f\"Yearly Trend of '{chosen_dataset}' Usage\")\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.ylabel(\"Count of Papers\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"No data found for dataset {chosen_dataset}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "We have:\n",
    "1. Fetched ~1000 ML papers from Semantic Scholar.\n",
    "2. Explored the data briefly.\n",
    "3. Extracted dataset mentions using a simple approach.\n",
    "4. Built a Neo4j graph to represent `Paper` and `Dataset` nodes, linked by `USES_DATASET`.\n",
    "5. Queried top datasets and a time-trend of a chosen dataset.\n",
    "\n",
    "### Next Steps\n",
    "- Enhance NLP extraction using advanced named-entity recognition or SciBERT.\n",
    "- Broaden the dataset list (or do an unsupervised approach to detect new dataset names).\n",
    "- Expand queries and visualizations, possibly integrating a front-end to explore the graph.\n",
    "\n",
    "**End of Notebook**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
